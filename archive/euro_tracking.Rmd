---
title: "Several Approaches to Finding Topics in News Article Headlines"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd('/Users/sirineerchal/Desktop/euro_tracking')
library(reticulate)
conda_create("r-reticulate")
conda_install("r-reticulate", packages = c("scipy","pandas","numpy","matplotlib"))
conda_install("r-reticulate","scikit-learn")
pd <- import("pandas")
np <- import('numpy')
scipy <- import('scipy')
matplotlib <- import('matplotlib')
sklearn <- import('sklearn')
```


```{r, include=FALSE}
```

```{r ruben}
setwd("/home/wrszemsrgyercqh/an_work/EU/")

library(tidyverse)
# Survey data and sociodemographic background data prep code unfortunately overwritten,
# but here is what was done to get from sdata.RData to dat_surv.RDS 
# See questionnaire for details on the quesions asked
#   - Panelist_id: Unique person identifier
#   - Country: Identifies a user's country
#   - reg_vote: Whether the user is registered to vote (v_8)
#   - tv_hh: Whether there is a TV in the hh (v_9)
#   - has.twitter: Whether the user has a twitter account (v_10-14)
#   - has.facebook: Whether the user has a facebook account (v_10-14)
#   - has.instagram: Whether the user has an instagram account (v_10-14)
#   - has.linkedin: Whether the user has a linkedin account (v_10-14)
#   - has.oth.smedia: Whether the user has additional social media accounts (v_10-14)
#   - linkedin: Whether the user intends to vote in the upcoming election (v_15)
#   - fr.pres.vote: Whether the user voted for Le Pen or Macron in 2017 French election (2nd round) (v_20)
#   - fr.yellow.v.supp: Whether the user supports (high) the yellow vest protests or not (low) (v_21)
#   - uk.2015.vote: Which party the user voted for in 2015 UK election (v_30)
#   - uk.brexit.vote: Whether the user voted for or against Brexit in referendum (v_31)
#   - de.wahlomat.use: Whether the user used Wahlomat in EU elections (DE only) (v_45)
#   - de.wahlomat.pty: Which party Wahlomat suggested (DE only) (v_46)
#   - voted: Whether the user voted in the EU election (v_9_2)
#   - change: Whether the user changed her mind about her choice (actual vote different from intention)
#   - intent_pty_cert: How sure the user is in his intended vote (v_17/v_27/v_38)
#   - undecided: Whether the user is undecided which party to vote for (v_16/v_26/v_37)
#   - v.pty.intent: Which party the user intends to vote (v_16/v_26/v_37)
#   - polinterest: Whether the user is interested in politics or not (v_25/v_36/v_44)
#   - polinterest.num: How much the user is interested in politics or not (v_25/v_36/v_44)
#   - leftmidright: Whether the user leans left, middle, or right (v_24/v_35/v_43)
#   - leftmidright.num: How much the user leans left, middle, or right (v_24/v_35/v_43)
#   - pty.feel.close: Which party the user feels closest to (v_18/v_28/v_39)
#   - last.election: Which party the user voted for in last national election (v_19/v_29/v_40)
#   - trust.EP: How much the user trusts the European parliament (v_22/v_32/v_41)
#   - trust.nat.pol: How much the user trusts the national parliament/other governmental institutions (v_23/v_33/v_42)

df.survey <- readRDS("./data/work/dat_surv.rds")

### In addition, panel.RData (somewhat incomplete) and panel_bis.RData were
# combined into a single file with as much sociodemographic info as possible
# sociodemo.rds

df.socdem <- readRDS("./data/work/sociodemo.rds")


setwd("/home/r_uma_2019/respondi_eu/")

load("./data/orig/URL1.RData")
load("./data/orig/visits.RData")
sdata <- readRDS("./data/work/dat_surv.RDS")

apps <- visits %>% 
  filter(d_kind == "app") %>% 
  rename(panelist_id = pseudonym,
         active_seconds = duration) %>% 
  select(-c(d_kind, pageviews, visit_id))
mobile.url <- visits %>% 
  filter(d_kind == "mobile") %>% 
  rename(panelist_id = pseudonym,
         active_seconds = duration) %>% 
  select(-c(d_kind, pageviews, visit_id))
pc.url <- URL1 %>% 
  select(-c(web_visits_id, id))

rm(visits, URL1)


sdata <- sdata %>% 
  select(panelist_id, country)

apps <- merge(apps, sdata,
              by="panelist_id",
              all = TRUE)
mobile.url <- merge(mobile.url, sdata,
              by="panelist_id",
              all = TRUE)
pc.url <- merge(pc.url, sdata,
              by="panelist_id",
              all = TRUE)

saveRDS(apps, "./data/work/apps.RDS")
saveRDS(mobile.url, "./data/work/m_url.RDS")
saveRDS(pc.url, "./data/work/pc_url.RDS")

pc.url <- readRDS("./data/work/pc_url.RDS")
m.url <- readRDS("./data/work/m_url.RDS")

pc.url <- pc.url %>% 
  select(-c(url))

URL1 <- rbind(pc.url, m.url)
rm(pc.url, m.url)


# Need to make sure we only have logs before election
URL1 <- URL1 %>% 
  filter(used_at<="2019-05-26 00:00:01")


# split "category" into four (max number of categories in "category") separate vars
cat1  <-  separate(URL1, category, c("cat1", NA, NA, NA), convert = TRUE)
cat1 <- cat1 %>% select(cat1)
cat2  <-  separate(URL1, category, c(NA, "cat2", NA, NA), convert = TRUE)
cat2 <- cat2 %>% select(cat2)
cat3  <-  separate(URL1, category, c(NA, NA, "cat3", NA), convert = TRUE)
cat3 <- cat3 %>% select(cat3)
cat4  <-  separate(URL1, category, c(NA, NA, NA, "cat4"), convert = TRUE)
cat4 <- cat4 %>% select(cat4)

categories <- cbind(cat1$cat1, cat2$cat2, cat3$cat3, cat4$cat4)
rm(cat1, cat2, cat3, cat4)
categories <- as.data.frame(categories)
URL1 <- cbind(URL1, categories)
rm(categories)
URL1 <- URL1  %>%
  select(-category) %>% 
  rename(cat1 = V1,
         cat2 = V2,
         cat3 = V3,
         cat4 = V4
  )

# Code NA as factor level
X <- apply(data.frame(URL1$cat1, URL1$cat2, URL1$cat3, URL1$cat4), 2,  fct_explicit_na)
X <- as.data.frame(X)
URL1$cat1 <- X$URL1.cat1
URL1$cat2 <- X$URL1.cat2
URL1$cat3 <- X$URL1.cat3
URL1$cat4 <- X$URL1.cat4
rm(X)


levels(URL1$cat1) <- c("missing", "abortion", "adult", "advertising", "alcoholandtobacco", "blacklist",
                       "blogsandpersonal", "business", "chatandmessaging","contentserver", "dating", "dating",
                       "deceptive", "drugs", "economyandfinance", "education", "entertainment", "foodandrecipes",
                       "foodandrecipes", "foodandrecipes", "gambling", "games", "health", "illegalcontent",
                       "informationtech", "jobrelated", "malicious", "mediasharing", "messageboardsandforums",
                       "newsandmedia", "newsandmedia", "parked", "personals", "proxyandfilteravoidance",
                       "realestate", "religion", "searchenginesandportals", "shopping", "socialnetworking",
                       "sports", "sports", "streamingmedia", "translation", "translation", "travel", "uncategorized",
                       "vehicles", "weapons")

levels(URL1$cat2) <- c("missing", "missing", "adult", "advertising", "alcoholandtobacco", "blogsandpersonal",
                       "business", "business", "chatandmessaging", "contentserver", "deceptive", "drugs", 
                       "economyandfinance", "education", "entertainment", "foodandrecipes", "gambling", 
                       "games", "health", "humor", "illegalcontent", "informationtech", "jobrelated",
                       "mediasharing", "messageboardsandforums", "newsandmedia", "parked", "personals",
                       "proxyandfilteravoidance", "realestate", "religion", "searchenginesandportals",
                       "shopping", "socialnetworking", "sports", "streamingmedia", "translation", "travel",
                       "vehicles", "virtualreality", "weapons")


levels(URL1$cat3) <- c("missing", "adult", "advertising", "alcoholandtobacco", "blogsandpersonal", "business",
                       "chatandmessaging", "drugs", "economyandfinance", "education", "entertainment", "foodandrecipes",
                       "gambling", "games", "health", "illegalcontent", "informationtech", "jobrelated", "mediasharing",
                       "messageboardsandforums", "newsandmedia", "parked", "personals", "proxyandfilteravoidance",
                       "realestate", "religion", "searchenginesandportals", "shopping", "socialnetworking", "sports",
                       "streamingmedia", "translation", "travel", "vehicles", "virtualreality", "weapons")


levels(URL1$cat4) <- c("missing", "alcoholandtobacco", "blogsandpersonal", "business", "education", "shopping", "vehicles")
##################################################################################################################
#### create variables from categories of URLs: calculate duration and frequency of use per category


duration1 <- URL1 %>%
  group_by(panelist_id, cat1) %>%
  summarise(
    d_per_cat1 = sum(active_seconds, na.rm = TRUE),
    n_per_cat1 = n()
  ) %>% 
  ungroup()

duration2 <- URL1 %>%
  group_by(panelist_id, cat2) %>%
  summarise(
    d_per_cat2 = sum(active_seconds, na.rm = TRUE),
    n_per_cat2 = n()
  ) %>% 
  ungroup()


duration3 <- URL1 %>%
  group_by(panelist_id, cat3) %>%
  summarise(
    d_per_cat3 = sum(active_seconds, na.rm = TRUE),
    n_per_cat3 = n()
  )  %>% 
  ungroup()


duration4 <- URL1 %>%
  group_by(panelist_id, cat4) %>%
  summarise(
    d_per_cat4 = sum(active_seconds, na.rm = TRUE),
    n_per_cat4 = n()
  )  %>% 
  ungroup()


# for duration 1
duration1.1 <- select(duration1, -n_per_cat1)
duration1.2 <- select(duration1, -d_per_cat1)


duration1.1 <- spread(duration1.1, cat1, d_per_cat1)
duration1.2 <- spread(duration1.2, cat1, n_per_cat1)

names1.1 <- names(duration1.1)
names1.2 <- names(duration1.2)

names1.1 <- paste0('d_' , names1.1, "1")
names1.2 <- paste0('n_' , names1.2, "1")

duration1.1 <- set_names(duration1.1, nm = names1.1)
duration1.2 <- set_names(duration1.2, nm = names1.2)
rm(names1.1,names1.2)


duration1.1 <- rename(duration1.1, panelist_id = d_panelist_id1)
duration1.2 <- rename(duration1.2, panelist_id = n_panelist_id1)

# for duration 2
duration2.1 <- select(duration2, -n_per_cat2)
duration2.2 <- select(duration2, -d_per_cat2)


duration2.1 <- spread(duration2.1, cat2, d_per_cat2)
duration2.2 <- spread(duration2.2, cat2, n_per_cat2)

names2.1 <- names(duration2.1)
names2.2 <- names(duration2.2)

names2.1 <- paste0('d_' , names2.1, "2")
names2.2 <- paste0('n_' , names2.2, "2")

duration2.1 <- set_names(duration2.1, nm = names2.1)
duration2.2 <- set_names(duration2.2, nm = names2.2)
rm(names2.1,names2.2)


duration2.1 <- rename(duration2.1, panelist_id = d_panelist_id2)
duration2.2 <- rename(duration2.2, panelist_id = n_panelist_id2)

# for duration 3
duration3.1 <- select(duration3, -n_per_cat3)
duration3.2 <- select(duration3, -d_per_cat3)


duration3.1 <- spread(duration3.1, cat3, d_per_cat3)
duration3.2 <- spread(duration3.2, cat3, n_per_cat3)

names3.1 <- names(duration3.1)
names3.2 <- names(duration3.2)

names3.1 <- paste0('d_' , names3.1, "3")
names3.2 <- paste0('n_' , names3.2, "3")

duration3.1 <- set_names(duration3.1, nm = names3.1)
duration3.2 <- set_names(duration3.2, nm = names3.2)
rm(names3.1,names3.2)


duration3.1 <- rename(duration3.1, panelist_id = d_panelist_id3)
duration3.2 <- rename(duration3.2, panelist_id = n_panelist_id3)

# for duration 4
duration4.1 <- select(duration4, -n_per_cat4)
duration4.2 <- select(duration4, -d_per_cat4)


duration4.1 <- spread(duration4.1, cat4, d_per_cat4)
duration4.2 <- spread(duration4.2, cat4, n_per_cat4)

names4.1 <- names(duration4.1)
names4.2 <- names(duration4.2)

names4.1 <- paste0('d_' , names4.1, "4")
names4.2 <- paste0('n_' , names4.2, "4")

duration4.1 <- set_names(duration4.1, nm = names4.1)
duration4.2 <- set_names(duration4.2, nm = names4.2)
rm(names4.1,names4.2)


duration4.1 <- rename(duration4.1, panelist_id = d_panelist_id4)
duration4.2 <- rename(duration4.2, panelist_id = n_panelist_id4)


duration1 <- merge(duration1.1, duration1.2,
                  by="panelist_id",
                  all = TRUE)
duration2 <- merge(duration2.1, duration2.2,
                  by="panelist_id",
                  all = TRUE)
duration3 <- merge(duration3.1, duration3.2,
                  by="panelist_id",
                  all = TRUE)
duration4 <- merge(duration4.1, duration4.2,
                  by="panelist_id",
                  all = TRUE)

rm(duration1.1,duration1.2,duration2.1,duration2.2,duration3.1,duration3.2,duration4.1,duration4.2)

duration <- merge(duration1, duration2,
                   by="panelist_id",
                   all = TRUE)
duration <- merge(duration, duration3,
                  by="panelist_id",
                  all = TRUE)
duration <- merge(duration, duration4,
                  by="panelist_id",
                  all = TRUE)

rm(duration1,duration2,duration3,duration4)

# total online time
total_dur <- URL1 %>%
  group_by(panelist_id) %>%
  summarise(
    d_total = sum(active_seconds, na.rm = TRUE),
    n_total = n()
  ) %>% 
  ungroup()

duration <- merge(total_dur, duration, by="panelist_id", all = TRUE)
rm(total_dur)

# calculate share of news_media/total online time
duration[is.na(duration)]  <- 0

duration$d.d_newsandmedia <- (duration$d_newsandmedia1 + duration$d_newsandmedia2 + duration$d_newsandmedia3)
duration$d.n_newsandmedia <- (duration$n_newsandmedia1 + duration$n_newsandmedia2 + duration$n_newsandmedia3)

duration$d.rd_newsandmedia <- duration$d.d_newsandmedia/duration$d_total
duration$d.rn_newsandmedia <- duration$d.n_newsandmedia/duration$n_total

summary(duration$d.rd_newsandmedia)

summary(duration$d.rn_newsandmedia)

X <- readRDS("./data/work/dat_surv.RDS")
X <- X %>% 
  select(c(panelist_id, country))

duration <- merge(duration, X, by="panelist_id", all.x = TRUE)

duration.de <- duration %>% 
  filter(country == "Germany") %>% 
  rename(d.total.dcat = d_total,
         n.total.dcat = n_total) %>% 
  select(-country)
duration.fr <- duration %>% 
  filter(country == "France") %>% 
  rename(d.total.dcat = d_total,
         n.total.dcat = n_total) %>% 
  select(-country)
duration.uk <- duration %>% 
  filter(country == "UK") %>% 
  rename(d.total.dcat = d_total,
         n.total.dcat = n_total) %>% 
  select(-country)


saveRDS(duration.uk, "./data/work/dur_per_cat_dom_uk.RDS")
saveRDS(duration.de, "./data/work/dur_per_cat_dom_de.RDS")
saveRDS(duration.fr, "./data/work/dur_per_cat_dom_fr.RDS")


rm(list = ls())

# do the same with apps
app <- readRDS("./data/work/apps.RDS")

# remove NAs
app <- app[!(is.na(app$domain)),]

app <- app %>% 
  filter(used_at<="2019-05-26 00:00:01")

app$category <- as.factor(app$category)

levels(app$category) <- c("adult", "entertainment", "business", "messaging", "finance", "games",
                       "health", "sports", "jobsandedu","lifestyle", "newsmedia", "preinstalled",
                       "search", "shopping", "social", "tools", "travel", "weather")


duration <- app %>%
  group_by(panelist_id, category) %>%
  summarise(
    d_per_cat = sum(active_seconds, na.rm = TRUE),
    n_per_cat = n()
  ) %>% 
  ungroup()

duration.1 <- select(duration, -n_per_cat)
duration.2 <- select(duration, -d_per_cat)


duration.1 <- spread(duration.1, category, d_per_cat)
duration.2 <- spread(duration.2, category, n_per_cat)

names.1 <- names(duration.1)
names.2 <- names(duration.2)

names.1 <- paste0('d_' , names.1)
names.2 <- paste0('n_' , names.2)

duration.1 <- set_names(duration.1, nm = names.1)
duration.2 <- set_names(duration.2, nm = names.2)
rm(names.1,names.2)


duration.1 <- rename(duration.1, panelist_id = d_panelist_id)
duration.2 <- rename(duration.2, panelist_id = n_panelist_id)


duration <- merge(duration.1, duration.2,
                   by="panelist_id",
                   all = TRUE)

rm(duration.1,duration.2)




# total online time
total_dur <- app %>%
  group_by(panelist_id) %>%
  summarise(
    d_total = sum(active_seconds, na.rm = TRUE),
    n_total = n()
  ) %>% 
  ungroup()

duration <- merge(total_dur, duration, by="panelist_id", all = TRUE)
rm(total_dur)

# calculate share of news_media/total online time
duration[is.na(duration)]  <- 0

duration$a.rd_newsandmedia <- duration$d_newsmedia/duration$d_total
duration$a.rn_newsandmedia <- duration$n_newsmedia/duration$n_total

summary(duration$a.rd_newsandmedia)

summary(duration$a.rn_newsandmedia)

X <- readRDS("./data/work/dat_surv.RDS")
X <- X %>% 
  select(c(panelist_id, country))

duration <- merge(duration, X, by="panelist_id", all.x = TRUE)

duration.de <- duration %>% 
  filter(country == "Germany") %>% 
  rename(d.total.acat = d_total,
         n.total.acat = n_total) %>% 
  select(-country)
duration.fr <- duration %>% 
  filter(country == "France") %>% 
  rename(d.total.acat = d_total,
         n.total.acat = n_total) %>% 
  select(-country)
duration.uk <- duration %>% 
  filter(country == "UK") %>% 
  rename(d.total.acat = d_total,
         n.total.acat = n_total) %>% 
  select(-country)


saveRDS(duration.uk, "./data/work/dur_per_cat_app_uk.RDS")
saveRDS(duration.de, "./data/work/dur_per_cat_app_de.RDS")
saveRDS(duration.fr, "./data/work/dur_per_cat_app_fr.RDS")


setwd("/home/r_uma_2019/respondi_eu/")

URL1 <- readRDS("./data/work/pc_url.RDS")

# Need to make sure we only have logs before election
URL1 <- URL1 %>% 
  filter(used_at<="2019-05-26 00:00:01")

# extract search terms from variable 'url'
URL1$search <- str_match(URL1$url, "(?:&|\\?)q=(.*?)&")[,2]
URL1$search2 <- str_match(URL1$url, "(?:&|\\?)url=(.*?)&")[,2]
URL1$search3 <- str_match(URL1$url, "(?:&|\\#)q=(.*?)&")[,2]
URL1$url.2 <- paste0(URL1$url, '¥')
URL1$search4 <- str_match(URL1$url.2, "(?:&|\\#|\\?)q=(.*?)¥")[,2]
# some special cases have different regular expressions to capture search terms 
# (e.g. use Yen at end of url to extract search terms that are at the very end

# set empty values to NA
URL1$search[URL1$search==""] <- NA
URL1$search2[URL1$search2==""] <- NA
URL1$search3[URL1$search3==""] <- NA
URL1$search4[URL1$search4==""] <- NA

# if first (broadest, most general) approach does not capture the search term, 
# overwrite with other, more specialised versions.
URL1$search[is.na(URL1$search)] <- URL1$search2[is.na(URL1$search)]
URL1$search[is.na(URL1$search)] <- URL1$search3[is.na(URL1$search)]
URL1$search[is.na(URL1$search)] <- URL1$search4[is.na(URL1$search)]

# Clean search terms.
URL1$search <- str_to_lower(URL1$search)
URL1$search <- str_replace_all(URL1$search, "\\+", " ")
URL1$search <- str_replace_all(URL1$search, "%20", " ")
URL1$search <- str_replace_all(URL1$search, "%c3%9f", "ß")
URL1$search <- str_replace_all(URL1$search, "%c3%b6|%c3%96", "ö")
URL1$search <- str_replace_all(URL1$search, "%c3%bc|%c3%9c", "ü")
URL1$search <- str_replace_all(URL1$search, "%c3%a4|%c3%84", "ä")
URL1$search <- str_replace_all(URL1$search, "%c3%a9|%c3%89", "é")
URL1$search <- str_replace_all(URL1$search, "%c3%a8|%C3%88", "è")
URL1$search <- str_replace_all(URL1$search, "%c3%ab|%c3%8b", "ë")
URL1$search <- str_replace_all(URL1$search, "%c3%aa|%c3%8a", "ê")
URL1$search <- str_replace_all(URL1$search, "%c3%a7|%c3%87", "ç")
URL1$search <- str_replace_all(URL1$search, "%c3%a6|%c3%86", "æ")
URL1$search <- str_replace_all(URL1$search, "%c3%a1", "á")
URL1$search <- str_replace_all(URL1$search, "%c3%a2|%c3%82", "â")
URL1$search <- str_replace_all(URL1$search, "%c3%a0|%c3%80", "à")
URL1$search <- str_replace_all(URL1$search, "%c3%ae|%c3%8e", "î")
URL1$search <- str_replace_all(URL1$search, "%c3%af|%c3%8f", "ï")
URL1$search <- str_replace_all(URL1$search, "%c5%93|%c5%92", "œ")
URL1$search <- str_replace_all(URL1$search, "%c3%b4|%c3%94", "ô")
URL1$search <- str_replace_all(URL1$search, "%c3%b9|%c3%99", "ù")
URL1$search <- str_replace_all(URL1$search, "%c3%bb|%c3%9b", "û")
URL1$search <- str_replace_all(URL1$search, "%c3%be|%c5%b8", "ÿ")
URL1$search <- str_replace_all(URL1$search, "%27", "'")
URL1$search <- str_replace_all(URL1$search, "%2c", ",")
URL1$search <- str_replace_all(URL1$search, "%3f", "?") # question
URL1$search <- str_replace_all(URL1$search, "%c2%a7", "§") # legal
URL1$search <- str_replace_all(URL1$search, "%24", "$") # dollar
URL1$search <- str_replace_all(URL1$search, "%2f", "/") # slash
URL1$search <- str_replace_all(URL1$search, "%26", "&") # and
URL1$search <- str_replace_all(URL1$search, "%2b", "+") # plus
URL1$search <- str_replace_all(URL1$search, "%3d", "=") # equal
URL1$search <- str_replace_all(URL1$search, "%40", "@") # ad
URL1$search <- str_replace_all(URL1$search, "%22", '"') # quote
URL1$search <- str_replace_all(URL1$search, "%3a", ':') # :
URL1$search <- str_replace_all(URL1$search, "%09", ' ') # tab
URL1$search <- str_replace_all(URL1$search, "%23", '#') # #  
URL1$search <- str_replace_all(URL1$search, "%0a", '') # line feed  
URL1$search <- str_replace_all(URL1$search, "%5b", '[') # bracket open
URL1$search <- str_replace_all(URL1$search, "%5d", ']') # bracket close  
URL1$search <- str_replace_all(URL1$search, "%5e", '^') # ^
URL1$search <- str_replace_all(URL1$search, "%5c", '\\') # \
URL1$search <- str_replace_all(URL1$search, "%7b", '{') # {
URL1$search <- str_replace_all(URL1$search, "%7c", '|') # |
URL1$search <- str_replace_all(URL1$search, "%7d", '}') # }
URL1$search <- str_replace_all(URL1$search, "%7e", '~') # tilde
URL1$search <- str_replace_all(URL1$search, "%3e", '>') # greater than
URL1$search <- str_replace_all(URL1$search, "%b4", "'") # acute accent
URL1$search <- str_replace_all(URL1$search, "%60", "`") # acute accent
URL1$search <- str_replace_all(URL1$search, "%25", '%') # percent


# flag search engines
# dummy variable for search engines in dataset (based on most common ones)
URL1$searchengine <- 0
URL1$searchengine[str_count(URL1$url, 
                            "google.de|google.com|bing.|asknow.|ecosia.|suche.gmx|
                                 suche.web|qwant.|search.avira.|nortonsafe.search.|wow.com|
                                 izito.|wolframalpha.|duckduckgo.|suche.aol.|baidu.|
                                 search.yahoo.|dogpile.|yippi.|ask.com|webcrawler.com|
                            search.com|ixquick.com|excite.com|info.com")>0]<- 1


# exclude entries that do not have anything to do with searching
URL1$searchengine[str_count(URL1$url, 
                            c("translate.google|googleads|maps.google|
                                   google.de/maps|google.com/maps/|google.com/earth|
                                   mail.google|play.google|keep.google|accounts.google|
                                   admin.google|analytics.google|drive.google|plus.google|
                                   support.google|news.google|chrome.google|calendar.google.|
                                   myaccount.google|docs.google|adwords.google|madeby.google|
                                   passwords.google|myactivity.google|productforums.google|
                                   sites.google|google.com/calendar|newsstand.google.|
                                   photos.google.|payments.google.|inbox.google.|
                                   business.google.|aboutme.google.|accounts.google.|
                                   google.com/gmail|withgoogle.com|developers.google.|
                                   researchbing.|google.com/chrome|script.google.|
                                   -seo-google.|google.de/adwords|hangouts.google.|
                                   contacts.google.|adssettings.google.|google.com/ads|
                                   groups.google.com|get.google.|source=bing|source=google"))>0]<- 0

URL1 <- URL1 %>% 
  select(-c(search2, search3, search4, url.2))


# separate cases that come from websearch page.
URL_small.searches <- URL1[URL1$searchengine == 1,c("panelist_id","url","used_at","active_seconds",
                                                         "search","country")]
URL_small.searches <- URL_small.searches[!(is.na(URL_small.searches$search)),]


saveRDS(URL_small.searches, file = "./data/work/dat_searchterms.RDS")

#### 

URL_small.searches <- readRDS(file = "./data/work/dat_searchterms.RDS")

URL_small.searches <- URL_small.searches[(which(nchar(URL_small.searches$search) > 1)),]


setwd("/home/r_uma_2019/respondi_eu/")

pc.url <- readRDS("./data/work/pc_url.RDS")
m.url <- readRDS("./data/work/m_url.RDS")

pc.url <- pc.url %>% 
  select(-c(url))

URL1 <- rbind(pc.url, m.url)
rm(pc.url, m.url)

# Need to make sure we only have logs before election
URL1 <- URL1 %>% 
  filter(used_at<="2019-05-26 00:00:01")

##################################################################################################################
#### for each country, create vars containing duration and frequency for most visited domains

## UK
URL1.uk <- URL1 %>%
  filter(country=="UK")
#how many participatns?
length(unique(URL1.uk$panelist_id))
#how many different domains?
length(unique(URL1.uk$domain))

### Limit to sites used by at least 3 different people
# calculate number of visits to each domain by each respondent
URL1.uk.keep <- URL1.uk %>% 
  group_by(domain, panelist_id) %>%
  summarise(
    n_per_dom = n()
  )

# replace n_per_dom with one
URL1.uk.keep$n_per_dom <- 1
URL1.uk.keep <- URL1.uk.keep[!is.na(URL1.uk.keep$domain),]

length((unique(URL1.uk.keep$domain)))
length((unique(URL1.uk.keep$panelist_id)))

# calculate again how many. now, its a count of how many diff participants per domain (because n_per_dom==1)
# delete if smaller/equal  3
URL1.uk.keep <- URL1.uk.keep %>% 
  group_by(domain) %>% 
  summarise(
    n_users = n()
  ) %>% 
  filter(n_users>=50) %>% 
  select(domain)
length(unique(URL1.uk.keep$domain))

# df withoout domains visited by fewer than 3 diff people
URL1.uk.reduced <- URL1.uk[URL1.uk$domain %in% URL1.uk.keep$domain, ]
rm(URL1.uk.keep)

# df to remove those visited fewer than 20 times and less than 60 seconds
# basis is already reduced data set URL1.uk.reduced
URL1.uk.keep.2 <- URL1.uk.reduced %>% 
  group_by(domain) %>%
  summarise(
    d_per_dom = sum(active_seconds, na.rm = TRUE),
    n_per_dom = n()
  ) %>% 
  filter(d_per_dom >= 1200 & n_per_dom >= 300) %>% 
  ungroup() %>% 
  select(domain) # keep only domains

length(unique(URL1.uk.keep.2$domain))
# delete from reduced dataset
URL1.uk.reduced <- URL1.uk.reduced[URL1.uk.reduced$domain %in% URL1.uk.keep.2$domain, ]
rm(URL1.uk.keep.2)
## finally,  calculate duration and frequency per domain (only those that remain)
URL1.uk.dur <- URL1.uk.reduced %>% 
  group_by(panelist_id, domain) %>%
  summarise(
    d_per_dom = sum(active_seconds, na.rm = TRUE),
    n_per_dom = n()
  ) %>%
  ungroup() 
# number of domains
length(unique(URL1.uk.dur$domain))
# number of participants
length(unique(URL1.uk.dur$panelist_id))

weird.ones.uk <- URL1.uk[(!(URL1.uk$panelist_id %in% URL1.uk.dur$panelist_id)), ]


## France
URL1.fr <- URL1 %>%
  filter(country=="France")
#how many participatns?
length(unique(URL1.fr$panelist_id))
#how many different domains?
length(unique(URL1.fr$domain))

### Limit to sites used by at least 3 different people
# calculate number of visits to each domain by each respondent
URL1.fr.keep <- URL1.fr %>% 
  group_by(domain, panelist_id) %>%
  summarise(
    n_per_dom = n()
  )

# replace n_per_dom with one
URL1.fr.keep$n_per_dom <- 1
URL1.fr.keep <- URL1.fr.keep[!is.na(URL1.fr.keep$domain),]

length((unique(URL1.fr.keep$domain)))
length((unique(URL1.fr.keep$panelist_id)))

# calculate again how many. now, its a count of how many diff participants per domain (because n_per_dom==1)
# delete if smaller/equal  3
URL1.fr.keep <- URL1.fr.keep %>% 
  group_by(domain) %>% 
  summarise(
    n_users = n()
  ) %>% 
  filter(n_users>=50) %>% 
  select(domain)
length(unique(URL1.fr.keep$domain))

# df withoout domains visited by fewer than 3 diff people
URL1.fr.reduced <- URL1.fr[URL1.fr$domain %in% URL1.fr.keep$domain, ]
rm(URL1.fr.keep)

# df to remove those visited fewer than 20 times and less than 60 seconds
# basis is already reduced data set URL1.fr.reduced
URL1.fr.keep.2 <- URL1.fr.reduced %>% 
  group_by(domain) %>%
  summarise(
    d_per_dom = sum(active_seconds, na.rm = TRUE),
    n_per_dom = n()
  ) %>% 
  filter(d_per_dom >= 1200 & n_per_dom >= 300) %>% 
  ungroup() %>% 
  select(domain) # keep only domains

length(unique(URL1.fr.keep.2$domain))
# delete from reduced dataset
URL1.fr.reduced <- URL1.fr.reduced[URL1.fr.reduced$domain %in% URL1.fr.keep.2$domain, ]
rm(URL1.fr.keep.2)
## finally,  calculate duration and frequency per domain (only those that remain)
URL1.fr.dur <- URL1.fr.reduced %>% 
  group_by(panelist_id, domain) %>%
  summarise(
    d_per_dom = sum(active_seconds, na.rm = TRUE),
    n_per_dom = n()
  ) %>%
  ungroup() 
# number of domains
length(unique(URL1.fr.dur$domain))
# number of participants
length(unique(URL1.fr.dur$panelist_id))

weird.ones.fr <- URL1.fr[(!(URL1.fr$panelist_id %in% URL1.fr.dur$panelist_id)), ]


## Germany
URL1.de <- URL1 %>%
  filter(country=="Germany")
#how many participatns?
length(unique(URL1.de$panelist_id))
#how many different domains?
length(unique(URL1.de$domain))

### Limit to sites used by at least 3 different people
# calculate number of visits to each domain by each respondent
URL1.de.keep <- URL1.de %>% 
  group_by(domain, panelist_id) %>%
  summarise(
    n_per_dom = n()
  )

# replace n_per_dom with one
URL1.de.keep$n_per_dom <- 1
URL1.de.keep <- URL1.de.keep[!is.na(URL1.de.keep$domain),]

length((unique(URL1.de.keep$domain)))
length((unique(URL1.de.keep$panelist_id)))

# calculate again how many. now, its a count of how many diff participants per domain (because n_per_dom==1)
# delete if smaller/equal  3
URL1.de.keep <- URL1.de.keep %>% 
  group_by(domain) %>% 
  summarise(
    n_users = n()
  ) %>% 
  filter(n_users>=50) %>% 
  select(domain)
length(unique(URL1.de.keep$domain))

# df withoout domains visited by fewer than 3 diff people
URL1.de.reduced <- URL1.de[URL1.de$domain %in% URL1.de.keep$domain, ]
rm(URL1.de.keep)

# df to remove those visited fewer than 20 times and less than 60 seconds
# basis is already reduced data set URL1.de.reduced
URL1.de.keep.2 <- URL1.de.reduced %>% 
  group_by(domain) %>%
  summarise(
    d_per_dom = sum(active_seconds, na.rm = TRUE),
    n_per_dom = n()
  ) %>% 
  filter(d_per_dom >= 1200 & n_per_dom >= 300) %>% 
  ungroup() %>% 
  select(domain) # keep only domains

length(unique(URL1.de.keep.2$domain))
# delete from reduced dataset
URL1.de.reduced <- URL1.de.reduced[URL1.de.reduced$domain %in% URL1.de.keep.2$domain, ]
rm(URL1.de.keep.2)
## finally,  calculate duration and frequency per domain (only those that remain)
URL1.de.dur <- URL1.de.reduced %>% 
  group_by(panelist_id, domain) %>%
  summarise(
    d_per_dom = sum(active_seconds, na.rm = TRUE),
    n_per_dom = n()
  ) %>%
  ungroup() 
# number of domains
length(unique(URL1.de.dur$domain))
# number of participants
length(unique(URL1.de.dur$panelist_id))

weird.ones.de <- URL1.de[(!(URL1.de$panelist_id %in% URL1.de.dur$panelist_id)), ]



### #remove what is obsolete
rm(URL1.de.reduced,URL1.fr.reduced,URL1.uk.reduced)



#######calculate time and frequency per domain

# DE
dur.de <- select(URL1.de.dur, -n_per_dom)
fre.de <- select(URL1.de.dur, -d_per_dom)


dur.de <- spread(dur.de, domain, d_per_dom)
fre.de <- spread(fre.de, domain, n_per_dom)

names.dur.de <- names(dur.de)
names.fre.de <- names(fre.de)

names.dur.de <- paste0('d_' , names.dur.de)
names.fre.de <- paste0('n_' , names.fre.de)

dur.de <- set_names(dur.de, nm = names.dur.de)
fre.de <- set_names(fre.de, nm = names.fre.de)
rm(names.dur.de,names.fre.de)


dur.de <- rename(dur.de, panelist_id = d_panelist_id)
fre.de <- rename(fre.de, panelist_id = n_panelist_id)

# FR
dur.fr <- select(URL1.fr.dur, -n_per_dom)
fre.fr <- select(URL1.fr.dur, -d_per_dom)


dur.fr <- spread(dur.fr, domain, d_per_dom)
fre.fr <- spread(fre.fr, domain, n_per_dom)

names.dur.fr <- names(dur.fr)
names.fre.fr <- names(fre.fr)

names.dur.fr <- paste0('d_' , names.dur.fr)
names.fre.fr <- paste0('n_' , names.fre.fr)

dur.fr <- set_names(dur.fr, nm = names.dur.fr)
fre.fr <- set_names(fre.fr, nm = names.fre.fr)
rm(names.dur.fr,names.fre.fr)


dur.fr <- rename(dur.fr, panelist_id = d_panelist_id)
fre.fr <- rename(fre.fr, panelist_id = n_panelist_id)

# UK
dur.uk <- select(URL1.uk.dur, -n_per_dom)
fre.uk <- select(URL1.uk.dur, -d_per_dom)


dur.uk <- spread(dur.uk, domain, d_per_dom)
fre.uk <- spread(fre.uk, domain, n_per_dom)

names.dur.uk <- names(dur.uk)
names.fre.uk <- names(fre.uk)

names.dur.uk <- paste0('d_' , names.dur.uk)
names.fre.uk <- paste0('n_' , names.fre.uk)

dur.uk <- set_names(dur.uk, nm = names.dur.uk)
fre.uk <- set_names(fre.uk, nm = names.fre.uk)
rm(names.dur.uk,names.fre.uk)


dur.uk <- rename(dur.uk, panelist_id = d_panelist_id)
fre.uk <- rename(fre.uk, panelist_id = n_panelist_id)



saveRDS(dur.de, "./data/work/dur_dom_de.RDS")
saveRDS(fre.de, "./data/work/fre_dom_de.RDS")

saveRDS(dur.fr, "./data/work/dur_dom_fr.RDS")
saveRDS(fre.fr, "./data/work/fre_dom_fr.RDS")

saveRDS(dur.uk, "./data/work/dur_dom_uk.RDS")
saveRDS(fre.uk, "./data/work/fre_dom_uk.RDS")


setwd("/home/r_uma_2019/respondi_eu/")

app <- readRDS("./data/work/apps.RDS")
app <- app[!(is.na(app$domain)),]

# Need to make sure we only have logs before election
app <- app %>% 
  filter(used_at<="2019-05-26 00:00:01")

# remove system apps
drop.app <- read.csv("./data/work/system_apps.csv")
drop.app <- drop.app %>% 
  rename(domain = n_packagenamestring) %>% 
  select(domain)
app <- app[!(app$domain %in% drop.app$domain), ]
rm(drop.app)

##################################################################################################################
#### for each country, create vars containing duration and frequency for most visited apps

## UK
app.uk <- app %>%
  filter(country=="UK")
#how many participatns?
length(unique(app.uk$panelist_id))
#how many different apps?
length(unique(app.uk$domain))

### Limit to apps used by at least 3 different people
# calculate number of visits to each app by each respondent
app.uk.keep <- app.uk %>% 
  group_by(domain, panelist_id) %>%
  summarise(
    n_per_app = n()
  )

# replace n_per_dom with one
app.uk.keep$n_per_app <- 1
app.uk.keep <- app.uk.keep[!is.na(app.uk.keep$domain),]

length((unique(app.uk.keep$domain)))
length((unique(app.uk.keep$panelist_id)))

# calculate again how many. now, its a count of how many diff participants per app (because n_per_dom==1)
# delete if smaller/equal  3
app.uk.keep <- app.uk.keep %>% 
  group_by(domain) %>% 
  summarise(
    n_users = n()
  ) %>% 
  filter(n_users>=50) %>% 
  select(domain)
length(unique(app.uk.keep$domain))

# df withoout apps visited by fewer than 3 diff people
app.uk.reduced <- app.uk[app.uk$domain %in% app.uk.keep$domain, ]
rm(app.uk.keep)

# df to remove those used fewer than 20 times and less than 60 seconds
# basis is already reduced data set app.uk.reduced
app.uk.keep.2 <- app.uk.reduced %>% 
  group_by(domain) %>%
  summarise(
    d_per_app = sum(active_seconds, na.rm = TRUE),
    n_per_app = n()
  ) %>% 
  filter(d_per_app >= 1200 & n_per_app >= 300) %>% 
  ungroup() %>% 
  select(domain) # keep only apps

length(unique(app.uk.keep.2$domain))
# delete from reduced dataset
app.uk.reduced <- app.uk.reduced[app.uk.reduced$domain %in% app.uk.keep.2$domain, ]
rm(app.uk.keep.2)
## finally,  calculate duration and frequency per app (only those that remain)
app.uk.dur <- app.uk.reduced %>% 
  group_by(panelist_id, domain) %>%
  summarise(
    d_per_app = sum(active_seconds, na.rm = TRUE),
    n_per_app = n()
  ) %>%
  ungroup() 
# number of apps
length(unique(app.uk.dur$domain))
# number of participants
length(unique(app.uk.dur$panelist_id))

weird.ones.uk <- app.uk[(!(app.uk$panelist_id %in% app.uk.dur$panelist_id)), ]


## France
app.fr <- app %>%
  filter(country=="France")
#how many participatns?
length(unique(app.fr$panelist_id))
#how many different apps?
length(unique(app.fr$domain))

### Limit to apps used by at least 3 different people
# calculate number of visits to each apps by each respondent
app.fr.keep <- app.fr %>% 
  group_by(domain, panelist_id) %>%
  summarise(
    n_per_app = n()
  )

# replace n_per_dom with one
app.fr.keep$n_per_dom <- 1
app.fr.keep <- app.fr.keep[!is.na(app.fr.keep$domain),]

length((unique(app.fr.keep$domain)))
length((unique(app.fr.keep$panelist_id)))

# calculate again how many. now, its a count of how many diff participants per apps (because n_per_dom==1)
# delete if smaller/equal  3
app.fr.keep <- app.fr.keep %>% 
  group_by(domain) %>% 
  summarise(
    n_users = n()
  ) %>% 
  filter(n_users>=50) %>% 
  select(domain)
length(unique(app.fr.keep$domain))

# df withoout apps visited by fewer than 3 diff people
app.fr.reduced <- app.fr[app.fr$domain %in% app.fr.keep$domain, ]
rm(app.fr.keep)

# df to remove those visited fewer than 20 times and less than 60 seconds
# basis is already reduced data set app.fr.reduced
app.fr.keep.2 <- app.fr.reduced %>% 
  group_by(domain) %>%
  summarise(
    d_per_app = sum(active_seconds, na.rm = TRUE),
    n_per_app = n()
  ) %>% 
  filter(d_per_app >= 1200 & n_per_app >= 300) %>% 
  ungroup() %>% 
  select(domain) # keep only apps

length(unique(app.fr.keep.2$domain))
# delete from reduced dataset
app.fr.reduced <- app.fr.reduced[app.fr.reduced$domain %in% app.fr.keep.2$domain, ]
rm(app.fr.keep.2)
## finally,  calculate duration and frequency per apps (only those that remain)
app.fr.dur <- app.fr.reduced %>% 
  group_by(panelist_id, domain) %>%
  summarise(
    d_per_app = sum(active_seconds, na.rm = TRUE),
    n_per_app = n()
  ) %>%
  ungroup() 
# number of apps
length(unique(app.fr.dur$domain))
# number of participants
length(unique(app.fr.dur$panelist_id))

weird.ones.fr <- app.fr[(!(app.fr$panelist_id %in% app.fr.dur$panelist_id)), ]


## Germany
app.de <- app %>%
  filter(country=="Germany")
#how many participatns?
length(unique(app.de$panelist_id))
#how many different app?
length(unique(app.de$domain))

### Limit to app used by at least 3 different people
# calculate number of visits to each app by each respondent
app.de.keep <- app.de %>% 
  group_by(domain, panelist_id) %>%
  summarise(
    n_per_app = n()
  )

# replace n_per_app with one
app.de.keep$n_per_dom <- 1
app.de.keep <- app.de.keep[!is.na(app.de.keep$domain),]

length((unique(app.de.keep$domain)))
length((unique(app.de.keep$panelist_id)))

# calculate again how many. now, its a count of how many diff participants per app (because n_per_dom==1)
# delete if smaller/equal  3
app.de.keep <- app.de.keep %>% 
  group_by(domain) %>% 
  summarise(
    n_users = n()
  ) %>% 
  filter(n_users>=50) %>% 
  select(domain)
length(unique(app.de.keep$domain))

# df withoout app visited by fewer than 3 diff people
app.de.reduced <- app.de[app.de$domain %in% app.de.keep$domain, ]
rm(app.de.keep)

# df to remove those visited fewer than 20 times and less than 60 seconds
# basis is already reduced data set app.de.reduced
app.de.keep.2 <- app.de.reduced %>% 
  group_by(domain) %>%
  summarise(
    d_per_app = sum(active_seconds, na.rm = TRUE),
    n_per_app = n()
  ) %>% 
  filter(d_per_app >= 1200 & n_per_app >= 300) %>% 
  ungroup() %>% 
  select(domain) # keep only app

length(unique(app.de.keep.2$domain))
# delete from reduced dataset
app.de.reduced <- app.de.reduced[app.de.reduced$domain %in% app.de.keep.2$domain, ]
rm(app.de.keep.2)
## finally,  calculate duration and frequency per app (only those that remain)
app.de.dur <- app.de.reduced %>% 
  group_by(panelist_id, domain) %>%
  summarise(
    d_per_app = sum(active_seconds, na.rm = TRUE),
    n_per_app = n()
  ) %>%
  ungroup() 
# number of domains
length(unique(app.de.dur$domain))
# number of participants
length(unique(app.de.dur$panelist_id))

weird.ones.de <- app.de[(!(app.de$panelist_id %in% app.de.dur$panelist_id)), ]



### #remove what is obsolete
rm(app.de.reduced,app.fr.reduced,app.uk.reduced)



#######calculate time and frequency per app

# DE
dur.de <- select(app.de.dur, -n_per_app)
fre.de <- select(app.de.dur, -d_per_app)


dur.de <- spread(dur.de, domain, d_per_app)
fre.de <- spread(fre.de, domain, n_per_app)

names.dur.de <- names(dur.de)
names.fre.de <- names(fre.de)

names.dur.de <- paste0('d_' , names.dur.de)
names.fre.de <- paste0('n_' , names.fre.de)

dur.de <- set_names(dur.de, nm = names.dur.de)
fre.de <- set_names(fre.de, nm = names.fre.de)
rm(names.dur.de,names.fre.de)


dur.de <- rename(dur.de, panelist_id = d_panelist_id)
fre.de <- rename(fre.de, panelist_id = n_panelist_id)

# FR
dur.fr <- select(app.fr.dur, -n_per_app)
fre.fr <- select(app.fr.dur, -d_per_app)


dur.fr <- spread(dur.fr, domain, d_per_app)
fre.fr <- spread(fre.fr, domain, n_per_app)

names.dur.fr <- names(dur.fr)
names.fre.fr <- names(fre.fr)

names.dur.fr <- paste0('d_' , names.dur.fr)
names.fre.fr <- paste0('n_' , names.fre.fr)

dur.fr <- set_names(dur.fr, nm = names.dur.fr)
fre.fr <- set_names(fre.fr, nm = names.fre.fr)
rm(names.dur.fr,names.fre.fr)


dur.fr <- rename(dur.fr, panelist_id = d_panelist_id)
fre.fr <- rename(fre.fr, panelist_id = n_panelist_id)

# UK
dur.uk <- select(app.uk.dur, -n_per_app)
fre.uk <- select(app.uk.dur, -d_per_app)


dur.uk <- spread(dur.uk, domain, d_per_app)
fre.uk <- spread(fre.uk, domain, n_per_app)

names.dur.uk <- names(dur.uk)
names.fre.uk <- names(fre.uk)

names.dur.uk <- paste0('d_' , names.dur.uk)
names.fre.uk <- paste0('n_' , names.fre.uk)

dur.uk <- set_names(dur.uk, nm = names.dur.uk)
fre.uk <- set_names(fre.uk, nm = names.fre.uk)
rm(names.dur.uk,names.fre.uk)


dur.uk <- rename(dur.uk, panelist_id = d_panelist_id)
fre.uk <- rename(fre.uk, panelist_id = n_panelist_id)


saveRDS(dur.de, "./data/work/dur_app_de.RDS")
saveRDS(fre.de, "./data/work/fre_app_de.RDS")

saveRDS(dur.fr, "./data/work/dur_app_fr.RDS")
saveRDS(fre.fr, "./data/work/fre_app_fr.RDS")

saveRDS(dur.uk, "./data/work/dur_app_uk.RDS")
saveRDS(fre.uk, "./data/work/fre_app_uk.RDS")


setwd("/home/an_work/EU/euro-tracking")

# Prep data for predictors from BERT encodings
  
library(FactoMineR)
library(ggplot2)
library(stringr)
library(distances)
library(plyr)
library(reshape2)

source("exploratory_topic_analysis_aux.R")


###### manage title encodings
### load encodings
te = read.csv2("result_bert_UK_final/title_mean_strategy.csv",stringsAsFactors = FALSE)
#temean = read.csv2("result_bert_UK_final/title_cls_strategy.csv",stringsAsFactors = FALSE)

### delete duplicates if any
te = te[which(!duplicated(te)),]

### convert vector dimensions columns to numeric
for (i in 6:ncol(te)){te[,i]=as.numeric(te[,i])}

### load url hits
load("url_UK.RData")

### check that all encoded titles come from UK url hits by respondents
summary(te$url%in%url_UK$url)

### merge url hits with encodings
euUK = merge(url_UK[,c("url","panelist_id","used_at","domain")],te,by="url",all.x=TRUE)

### keep only one url hit by respondent
### to prevent overweighting articles manically read many many times by the same person
names(euUK)[which(names(euUK)=="panelist_id")]="pseudonym"
euUK = euUK[which(!duplicated(euUK[,c("pseudonym","url")])),]

### check results
print(paste("Successfully encoded urls:",floor(length(which(!is.na(euUK$X767)))/nrow(euUK)*100),"%"))

### filter out respondents for whom we don't have much info
### filter out articles whose titles have not been encoded
euUK$count = 1
individual.counts = aggregate(euUK$count,by=list(euUK$pseudonym),FUN=sum)
summary(individual.counts)
euUK = euUK[which(euUK$pseudonym %in% individual.counts$Group.1[which(individual.counts$x>10)]),]
euUK = na.omit(euUK)



####################
### PCA on BERT encoding
### interpreting dimensions and "who reads what"


for.PCA = euUK[,which(str_detect(names(euUK),"^X"))]
res = PCA(for.PCA)
dim = as.data.frame(res$ind$coord)
dim$index = 1:nrow(dim)
dim$pseudonym = euUK$pseudonym

### interpreting PCA dimensions
### dimension 1: football vs scary news (abuse & crime)
interpret(dim,1,euUK)
### dimension 2: TV series vs Brexit
interpret(dim,2,euUK)
### dimension 3: Europen Elections vs (sex) crime
interpret(dim,3,euUK)
### dimension 4: Local news vs celebrities
interpret(dim,4,euUK)
### dimension 5: sports (non football, mixed with some other international stuff) vs deals (money related, business)
interpret(dim,5,euUK)

load("survey_data.RData")
mix = merge(dim,survey_data,by="pseudonym",all.x=TRUE)
mix$v_31 = factor(mix$v_31)
levels(mix$v_31) = c(NA,NA,NA,"remain","leave")
names(mix)[which(names(mix)=="v_31")]="referendum_vote"

### variable "change" says whether they changed their mind in the last European elections (pre-election vs post-election survey)
### variable "referendum_vote" says what they said they voted (if they voted) to the 2016 EU membership referendum

### referendum x topics: leave people are more into scary news, more into brexit news, and more into sex crime as well
aggregate(mix[,paste0("Dim.",c(1:5))],by=list(mix$referendum_vote),FUN=mean)

# making a picture
# each point correspond to an article, it is colored according to the behavior of the person who read it
# articles on the left are about football, they are mostly read by remain people, articles to the right are scary, they are read by leave people
# articles at the bottom are about TV series, mostly on the BBC, they are read by remain people
# articles at the top are about sex crimes, they are read by leave people
q = ggplot(transform(mix[, c("Dim.1","Dim.2")], cluster = mix$referendum_vote), 
           aes(x = Dim.1, y = Dim.2, colour = cluster)) +
  geom_point() + scale_colour_manual(values=c("purple", "orange"))

q

### change in voting intention: people who changed are much less into brexit news, and they are also more into celebs than local news
### ---> people who change their mind are less interested in both national and local politics
aggregate(mix[,paste0("Dim.",c(1:5))],by=list(mix$change),FUN=mean)

# making a picture
# each point correspond to an article, it is colored according to the behavior of the person who read it
# articles at the bottom and to the right are politics related (Brexit / local news), they are mostly read by people who don't change their mind
q = ggplot(transform(mix[, c("Dim.2","Dim.4")], cluster = mix$change), 
           aes(x = Dim.2, y = Dim.4, colour = cluster)) +
           geom_point() + scale_colour_manual(values=c("purple", "orange","gray100","gray100"))

q


##########################
############# VERY MESSY STUFF JUST FOR ME TO BE ASHAMED OF
############# CLUSTERING ARTICLES

################# determination du nombre optimal de cluster
encodings = euUK[,which(str_detect(names(euUK),"^X"))]


wss <- 2:10
for (i in 2:10) wss[i-1] <- sum(kmeans(encodings,
                                       centers=i)$withinss)
plot(2:10, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")

################# creation des clusters dim 1 et 2 après normalisation

nbclust <- 9
kmeans.comment <- kmeans(encodings,nbclust)
cluster.kmeans <- as.factor(kmeans.comment$cluster)
table(cluster.kmeans)
centers = kmeans.comment$centers
nrow(centers)

tyti = find.typical.titles(encodings,centers,z=50,euUK)
View(tyti)

ac = data.frame(clusters = cluster.kmeans,pseudonym = euUK$pseudonym)
levels(ac$clusters) =c("general news","sports","Brexit","TV series","money","celebs","crime","football","entertainment")
ac$count = 1
ac2 = aggregate(ac$count,by=list(ac$pseudonym,ac$clusters),FUN=sum,drop=FALSE)
names(ac2) = c("pseudonym","cluster","count")
head(ac2)



# Specify id.vars: the variables to keep but not split apart on
ac2w <- dcast(ac2, pseudonym ~ cluster, value.var="count")
ac2w$total = rowSums(ac2w[,2:ncol(ac2w)])
for (i in 2:(ncol(ac2w)-1)){
  ac2w[,i] = ac2w[,i] / ac2w$total
}

mix2 = merge(ac2w,survey_data,by="pseudonym",all.x=TRUE)
mix2$v_31 = factor(mix2$v_31)
levels(mix2$v_31) = c(NA,NA,NA,"remain","leave")
names(mix2)[which(names(mix2)=="v_31")]="referendum_vote"
mix2$change = factor(mix2$change)
levels(mix2$change) = c("changed vote","did not change",NA,NA)

mix2s = na.omit(mix2[,c("pseudonym","referendum_vote",as.character(levels(ac$clusters)))])
mix2l = melt(mix2s, id.vars=c("pseudonym", "referendum_vote"))
head(mix2l)


df2 <- data_summary(mix2l, varname="value", 
                    groupnames=c("variable", "referendum_vote"))
df2$referendum_vote=as.factor(df2$referendum_vote)


p<- ggplot(df2, aes(x=variable, y=value, fill=referendum_vote)) + 
  geom_bar(stat="identity", color="black", 
           position=position_dodge())
p

##########
mix3s = na.omit(mix2[,c("pseudonym","change",as.character(levels(ac$clusters)))])
mix3l = melt(mix3s, id.vars=c("pseudonym", "change"))

df2 <- data_summary(mix3l, varname="value", 
                    groupnames=c("variable", "change"))
df2$change =as.factor(df2$change)


p<- ggplot(df2, aes(x=variable, y=value, fill=change)) + 
  geom_bar(stat="identity", color="black", 
           position=position_dodge())
print(p)

# encoded news titles for DE FR and UK saved in title_encodings_DE.csv, title_encodings_FR.csv and title_encodings_UK.csv

data_summary <- function(data, varname, groupnames){
  require(plyr)
  summary_func <- function(x, col){
    c(mean = mean(x[[col]], na.rm=TRUE),
      sd = sd(x[[col]], na.rm=TRUE))
  }
  data_sum<-ddply(data, groupnames, .fun=summary_func,
                  varname)
  data_sum <- rename(data_sum, c("mean" = varname))
  return(data_sum)
}



interpret = function(PCA.coordinates,selected.dimension,encodings,nn=10){
  redundancies = which(duplicated(encodings$title_))
  PCA.coordinates$index = 1:nrow(encodings)
  PCA.coordinates = PCA.coordinates[-redundancies,]
  PCA.coordinates = PCA.coordinates[order(PCA.coordinates[,selected.dimension]),]
  extremes = c(1:nn,(nrow(PCA.coordinates)-(nn-1)):nrow(PCA.coordinates))
  target = PCA.coordinates$index[extremes]
  output = data.frame(sign = c(rep("-",nn),rep("+",nn)),title = encodings$title_filtered[target],coordinate = PCA.coordinates[extremes,selected.dimension] )
  return(output)
}

find.typical.titles = function(dims,centers,z,uw3){
  require(distances)
  dims2 = rbind(dims,centers)
  dd = distances(dims2)
  dd2 = as.data.frame(distance_columns(dd, c((nrow(dims)+1):(nrow(dims)+nrow(centers))),  row_indices = c(1:nrow(dims))))
  dd2$index = 1:nrow(dd2)
  
  closest = lapply(c(1:nrow(centers)),find.closest,dd2=dd2,z)
  closest = do.call(cbind,closest)
  
  closest.titles = closest
  
  for (i in c(1:ncol(closest))){
    closest.titles[,i]=uw3$title[closest[,i]]
  }
  
  return(closest.titles)
}


setwd("/home/wrszemsrgyercqh/an_work/EU/")


library(FactoMineR)
library(ggplot2)
library(stringr)
library(distances)
# library(plyr)
library(reshape2)
# 
library(data.table)
source("./euro-tracking/src/01_prep/0_2_6_1_helper_fun_topic_extraction.R")

# Get clusters of news articles from BERT encodings --- DE
###### manage title encodings
### load encodings
te = fread("./data/work/title_encodings_DE.csv",stringsAsFactors = FALSE)
te <- as.data.frame(te)

# names(te) <- substring(names(te), 3)
# names(te) = substr(names(te),1,nchar(names(te))-1)

### delete duplicates if any
# te = te[which(!duplicated(te)),]

### convert vector dimensions columns to numeric
for (i in 12:ncol(te)){te[,i]=as.numeric(te[,i])}

### merge url hits with encodings
euDE = te

### keep only one url hit by respondent
### to prevent overweighting articles manically read many many times by the same person
names(euDE)[which(names(euDE)=="panelist_id")]="pseudonym"
euDE = euDE[which(!duplicated(euDE[,c("pseudonym","url")])),]

### filter out respondents for whom we don't have much info
### filter out articles whose titles have not been encoded
euDE$count = 1
individual.counts = aggregate(euDE$count,by=list(euDE$pseudonym),FUN=sum)
summary(individual.counts)
# euDE = euDE[which(euDE$pseudonym %in% individual.counts$Group.1[which(individual.counts$x>10)]),]
euDE = na.omit(euDE)

####################
### PCA on BERT encoding
### interpreting dimensions and "who reads what"
for.PCA = euDE[,which(str_detect(names(euDE),"^X"))]
res = PCA(for.PCA)
dim = as.data.frame(res$ind$coord)
dim$index = 1:nrow(dim)
dim$pseudonym = euDE$pseudonym

################# determination du nombre optimal de cluster
encodings = euDE[,which(str_detect(names(euDE),"^X"))]
wss <- 2:50
for (i in 2:50) wss[i-1] <- sum(kmeans(encodings,
                                       centers=i)$withinss)
plot(2:50, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")
################# creation des clusters dim 1 et 2 aprÃ¨s normalisation

nbclust <- 50
kmeans.comment <- kmeans(encodings,nbclust)
cluster.kmeans <- as.factor(kmeans.comment$cluster)
table(cluster.kmeans)
centers = kmeans.comment$centers
nrow(centers)

ac = data.frame(clusters = cluster.kmeans,pseudonym = euDE$pseudonym)
ac$count = 1
ac$used_at <- euDE$used_at
ac$url <- euDE$url
ac$active_seconds <- euDE$active_seconds
ac$active_seconds <- as.numeric(ac$active_seconds)

AC.2 <- ac %>% 
  group_by(pseudonym, clusters) %>% 
  dplyr::summarise(
    n_per_clust = n(),
    d_per_clust = sum(active_seconds, na.rm = TRUE))

AC.n <- spread(AC.2, clusters, n_per_clust)
AC.d <- spread(AC.2, clusters, d_per_clust)


AC.n <- AC.n %>%
  group_by(pseudonym) %>% 
  dplyr::summarise(
    clus1 = sum(`1`, na.rm = TRUE),
    clus2 = sum(`2`, na.rm = TRUE),
    clus3 = sum(`3`, na.rm = TRUE),
    clus4 = sum(`4`, na.rm = TRUE),
    clus5 = sum(`5`, na.rm = TRUE),
    clus6 = sum(`6`, na.rm = TRUE),
    clus7 = sum(`7`, na.rm = TRUE),
    clus8 = sum(`8`, na.rm = TRUE),
    clus9 = sum(`9`, na.rm = TRUE),
    clus10 = sum(`10`, na.rm = TRUE),
    clus11 = sum(`11`, na.rm = TRUE),
    clus12 = sum(`12`, na.rm = TRUE),
    clus13 = sum(`13`, na.rm = TRUE),
    clus14 = sum(`14`, na.rm = TRUE),
    clus15 = sum(`15`, na.rm = TRUE),
    clus16 = sum(`16`, na.rm = TRUE),
    clus17 = sum(`17`, na.rm = TRUE),
    clus18 = sum(`18`, na.rm = TRUE),
    clus19 = sum(`19`, na.rm = TRUE),
    clus20 = sum(`20`, na.rm = TRUE),
    clus21 = sum(`21`, na.rm = TRUE),
    clus22 = sum(`22`, na.rm = TRUE),
    clus23 = sum(`23`, na.rm = TRUE),
    clus24 = sum(`24`, na.rm = TRUE),
    clus25 = sum(`25`, na.rm = TRUE),
    clus26 = sum(`26`, na.rm = TRUE),
    clus27 = sum(`27`, na.rm = TRUE),
    clus28 = sum(`28`, na.rm = TRUE),
    clus29 = sum(`29`, na.rm = TRUE),
    clus30 = sum(`30`, na.rm = TRUE),
    clus31 = sum(`31`, na.rm = TRUE),
    clus32 = sum(`32`, na.rm = TRUE),
    clus33= sum(`33`, na.rm = TRUE),
    clus34= sum(`34`, na.rm = TRUE),
    clus35= sum(`35`, na.rm = TRUE),
    clus36= sum(`36`, na.rm = TRUE),
    clus37= sum(`37`, na.rm = TRUE),
    clus38= sum(`38`, na.rm = TRUE),
    clus39= sum(`39`, na.rm = TRUE),
    clus40= sum(`40`, na.rm = TRUE),
    clus41= sum(`41`, na.rm = TRUE),
    clus42= sum(`42`, na.rm = TRUE),
    clus43= sum(`43`, na.rm = TRUE),
    clus44= sum(`44`, na.rm = TRUE),
    clus45= sum(`45`, na.rm = TRUE),
    clus46= sum(`46`, na.rm = TRUE),
    clus47= sum(`47`, na.rm = TRUE),
    clus48= sum(`48`, na.rm = TRUE),
    clus49= sum(`49`, na.rm = TRUE),
    clus50= sum(`50`, na.rm = TRUE)
  )

AC.d <- AC.d %>%
  group_by(pseudonym) %>% 
  summarise(
    clus1 = sum(`1`, na.rm = TRUE),
    clus2 = sum(`2`, na.rm = TRUE),
    clus3 = sum(`3`, na.rm = TRUE),
    clus4 = sum(`4`, na.rm = TRUE),
    clus5 = sum(`5`, na.rm = TRUE),
    clus6 = sum(`6`, na.rm = TRUE),
    clus7 = sum(`7`, na.rm = TRUE),
    clus8 = sum(`8`, na.rm = TRUE),
    clus9 = sum(`9`, na.rm = TRUE),
    clus10 = sum(`10`, na.rm = TRUE),
    clus11 = sum(`11`, na.rm = TRUE),
    clus12 = sum(`12`, na.rm = TRUE),
    clus13 = sum(`13`, na.rm = TRUE),
    clus14 = sum(`14`, na.rm = TRUE),
    clus15 = sum(`15`, na.rm = TRUE),
    clus16 = sum(`16`, na.rm = TRUE),
    clus17 = sum(`17`, na.rm = TRUE),
    clus18 = sum(`18`, na.rm = TRUE),
    clus19 = sum(`19`, na.rm = TRUE),
    clus20 = sum(`20`, na.rm = TRUE),
    clus21 = sum(`21`, na.rm = TRUE),
    clus22 = sum(`22`, na.rm = TRUE),
    clus23 = sum(`23`, na.rm = TRUE),
    clus24 = sum(`24`, na.rm = TRUE),
    clus25 = sum(`25`, na.rm = TRUE),
    clus26 = sum(`26`, na.rm = TRUE),
    clus27 = sum(`27`, na.rm = TRUE),
    clus28 = sum(`28`, na.rm = TRUE),
    clus29 = sum(`29`, na.rm = TRUE),
    clus30 = sum(`30`, na.rm = TRUE),
    clus31 = sum(`31`, na.rm = TRUE),
    clus32 = sum(`32`, na.rm = TRUE),
    clus33= sum(`33`, na.rm = TRUE),
    clus34= sum(`34`, na.rm = TRUE),
    clus35= sum(`35`, na.rm = TRUE),
    clus36= sum(`36`, na.rm = TRUE),
    clus37= sum(`37`, na.rm = TRUE),
    clus38= sum(`38`, na.rm = TRUE),
    clus39= sum(`39`, na.rm = TRUE),
    clus40= sum(`40`, na.rm = TRUE),
    clus41= sum(`41`, na.rm = TRUE),
    clus42= sum(`42`, na.rm = TRUE),
    clus43= sum(`43`, na.rm = TRUE),
    clus44= sum(`44`, na.rm = TRUE),
    clus45= sum(`45`, na.rm = TRUE),
    clus46= sum(`46`, na.rm = TRUE),
    clus47= sum(`47`, na.rm = TRUE),
    clus48= sum(`48`, na.rm = TRUE),
    clus49= sum(`49`, na.rm = TRUE),
    clus50= sum(`50`, na.rm = TRUE)
  )

saveRDS(AC.d, "./data/work/dcluster_DE.RDS")
saveRDS(AC.n, "./data/work/ncluster_DE.RDS")

rm(list = ls())

#################################################################################
#### FRANCE

###### manage title encodings
### load encodings
te = read.csv2("./data/work/title_encodings_FR.csv",stringsAsFactors = FALSE)

### delete duplicates if any
te = te[which(!duplicated(te)),]

### convert vector dimensions columns to numeric
for (i in 12:ncol(te)){te[,i]=as.numeric(te[,i])}

### merge url hits with encodings
euFR = te

### keep only one url hit by respondent
### to prevent overweighting articles manically read many many times by the same person
names(euFR)[which(names(euFR)=="panelist_id")]="pseudonym"
euFR = euFR[which(!duplicated(euFR[,c("pseudonym","url")])),]

### filter out respondents for whom we don't have much info
### filter out articles whose titles have not been encoded
euFR$count = 1
individual.counts = aggregate(euFR$count,by=list(euFR$pseudonym),FUN=sum)
summary(individual.counts)
euFR = euFR[which(euFR$pseudonym %in% individual.counts$Group.1[which(individual.counts$x>10)]),]
euFR = na.omit(euFR)

####################
### PCA on BERT encoding
### interpreting dimensions and "who reads what"
for.PCA = euFR[,which(str_detect(names(euFR),"^X"))]
res = PCA(for.PCA)
dim = as.data.frame(res$ind$coord)
dim$index = 1:nrow(dim)
dim$pseudonym = euFR$pseudonym

################# determination du nombre optimal de cluster
encodings = euFR[,which(str_detect(names(euFR),"^X"))]
wss <- 2:50
for (i in 2:50) wss[i-1] <- sum(kmeans(encodings,
                                       centers=i)$withinss)

################# creation des clusters dim 1 et 2 aprÃ¨s normalisation

nbclust <- 50
kmeans.comment <- kmeans(encodings,nbclust)
cluster.kmeans <- as.factor(kmeans.comment$cluster)
table(cluster.kmeans)
centers = kmeans.comment$centers
nrow(centers)

ac = data.frame(clusters = cluster.kmeans,pseudonym = euFR$pseudonym)
ac$count = 1
ac$used_at <- euFR$used_at
ac$url <- euFR$url
ac$active_seconds <- euFR$active_seconds
ac$active_seconds <- as.numeric(ac$active_seconds)

AC.2 <- ac %>% 
  group_by(pseudonym, clusters) %>% 
  summarise(
    n_per_clust = n(),
    d_per_clust = sum(active_seconds, na.rm = TRUE))

AC.n <- spread(AC.2, clusters, n_per_clust)
AC.d <- spread(AC.2, clusters, d_per_clust)


AC.n <- AC.n %>%
  group_by(pseudonym) %>% 
  summarise(
    clus1 = sum(`1`, na.rm = TRUE),
    clus2 = sum(`2`, na.rm = TRUE),
    clus3 = sum(`3`, na.rm = TRUE),
    clus4 = sum(`4`, na.rm = TRUE),
    clus5 = sum(`5`, na.rm = TRUE),
    clus6 = sum(`6`, na.rm = TRUE),
    clus7 = sum(`7`, na.rm = TRUE),
    clus8 = sum(`8`, na.rm = TRUE),
    clus9 = sum(`9`, na.rm = TRUE),
    clus10 = sum(`10`, na.rm = TRUE),
    clus11 = sum(`11`, na.rm = TRUE),
    clus12 = sum(`12`, na.rm = TRUE),
    clus13 = sum(`13`, na.rm = TRUE),
    clus14 = sum(`14`, na.rm = TRUE),
    clus15 = sum(`15`, na.rm = TRUE),
    clus16 = sum(`16`, na.rm = TRUE),
    clus17 = sum(`17`, na.rm = TRUE),
    clus18 = sum(`18`, na.rm = TRUE),
    clus19 = sum(`19`, na.rm = TRUE),
    clus20 = sum(`20`, na.rm = TRUE),
    clus21 = sum(`21`, na.rm = TRUE),
    clus22 = sum(`22`, na.rm = TRUE),
    clus23 = sum(`23`, na.rm = TRUE),
    clus24 = sum(`24`, na.rm = TRUE),
    clus25 = sum(`25`, na.rm = TRUE),
    clus26 = sum(`26`, na.rm = TRUE),
    clus27 = sum(`27`, na.rm = TRUE),
    clus28 = sum(`28`, na.rm = TRUE),
    clus29 = sum(`29`, na.rm = TRUE),
    clus30 = sum(`30`, na.rm = TRUE),
    clus31 = sum(`31`, na.rm = TRUE),
    clus32 = sum(`32`, na.rm = TRUE),
    clus33= sum(`33`, na.rm = TRUE),
    clus34= sum(`34`, na.rm = TRUE),
    clus35= sum(`35`, na.rm = TRUE),
    clus36= sum(`36`, na.rm = TRUE),
    clus37= sum(`37`, na.rm = TRUE),
    clus38= sum(`38`, na.rm = TRUE),
    clus39= sum(`39`, na.rm = TRUE),
    clus40= sum(`40`, na.rm = TRUE),
    clus41= sum(`41`, na.rm = TRUE),
    clus42= sum(`42`, na.rm = TRUE),
    clus43= sum(`43`, na.rm = TRUE),
    clus44= sum(`44`, na.rm = TRUE),
    clus45= sum(`45`, na.rm = TRUE),
    clus46= sum(`46`, na.rm = TRUE),
    clus47= sum(`47`, na.rm = TRUE),
    clus48= sum(`48`, na.rm = TRUE),
    clus49= sum(`49`, na.rm = TRUE),
    clus50= sum(`50`, na.rm = TRUE)
  )

AC.d <- AC.d %>%
  group_by(pseudonym) %>% 
  summarise(
    clus1 = sum(`1`, na.rm = TRUE),
    clus2 = sum(`2`, na.rm = TRUE),
    clus3 = sum(`3`, na.rm = TRUE),
    clus4 = sum(`4`, na.rm = TRUE),
    clus5 = sum(`5`, na.rm = TRUE),
    clus6 = sum(`6`, na.rm = TRUE),
    clus7 = sum(`7`, na.rm = TRUE),
    clus8 = sum(`8`, na.rm = TRUE),
    clus9 = sum(`9`, na.rm = TRUE),
    clus10 = sum(`10`, na.rm = TRUE),
    clus11 = sum(`11`, na.rm = TRUE),
    clus12 = sum(`12`, na.rm = TRUE),
    clus13 = sum(`13`, na.rm = TRUE),
    clus14 = sum(`14`, na.rm = TRUE),
    clus15 = sum(`15`, na.rm = TRUE),
    clus16 = sum(`16`, na.rm = TRUE),
    clus17 = sum(`17`, na.rm = TRUE),
    clus18 = sum(`18`, na.rm = TRUE),
    clus19 = sum(`19`, na.rm = TRUE),
    clus20 = sum(`20`, na.rm = TRUE),
    clus21 = sum(`21`, na.rm = TRUE),
    clus22 = sum(`22`, na.rm = TRUE),
    clus23 = sum(`23`, na.rm = TRUE),
    clus24 = sum(`24`, na.rm = TRUE),
    clus25 = sum(`25`, na.rm = TRUE),
    clus26 = sum(`26`, na.rm = TRUE),
    clus27 = sum(`27`, na.rm = TRUE),
    clus28 = sum(`28`, na.rm = TRUE),
    clus29 = sum(`29`, na.rm = TRUE),
    clus30 = sum(`30`, na.rm = TRUE),
    clus31 = sum(`31`, na.rm = TRUE),
    clus32 = sum(`32`, na.rm = TRUE),
    clus33= sum(`33`, na.rm = TRUE),
    clus34= sum(`34`, na.rm = TRUE),
    clus35= sum(`35`, na.rm = TRUE),
    clus36= sum(`36`, na.rm = TRUE),
    clus37= sum(`37`, na.rm = TRUE),
    clus38= sum(`38`, na.rm = TRUE),
    clus39= sum(`39`, na.rm = TRUE),
    clus40= sum(`40`, na.rm = TRUE),
    clus41= sum(`41`, na.rm = TRUE),
    clus42= sum(`42`, na.rm = TRUE),
    clus43= sum(`43`, na.rm = TRUE),
    clus44= sum(`44`, na.rm = TRUE),
    clus45= sum(`45`, na.rm = TRUE),
    clus46= sum(`46`, na.rm = TRUE),
    clus47= sum(`47`, na.rm = TRUE),
    clus48= sum(`48`, na.rm = TRUE),
    clus49= sum(`49`, na.rm = TRUE),
    clus50= sum(`50`, na.rm = TRUE)
  )


saveRDS(AC.d, "./data/work/dcluster_FR.RDS")
saveRDS(AC.n, "./data/work/ncluster_FR.RDS")

rm(list = ls())


##############################################################
##### UK
###### manage title encodings
### load encodings
te = read.csv2("./data/work/title_encodings_UK.csv",stringsAsFactors = FALSE)

### delete duplicates if any
te = te[which(!duplicated(te)),]

### convert vector dimensions columns to numeric
for (i in 6:ncol(te)){te[,i]=as.numeric(te[,i])}

### load url hits
load("./data/work/url_UK.RData")

### check that all encoded titles come from UK url hits by respondents
summary(te$url%in%url_UK$url)

### merge url hits with encodings
euUK = merge(url_UK[,c("url","panelist_id","used_at","domain")],te,by="url",all.x=TRUE)

### keep only one url hit by respondent
### to prevent overweighting articles manically read many many times by the same person
names(euUK)[which(names(euUK)=="panelist_id")]="pseudonym"
euUK = euUK[which(!duplicated(euUK[,c("pseudonym","url")])),]

### check results
print(paste("Successfully encoded urls:",floor(length(which(!is.na(euUK$X767)))/nrow(euUK)*100),"%"))

### filter out respondents for whom we don't have much info
### filter out articles whose titles have not been encoded
euUK$count = 1
individual.counts = aggregate(euUK$count,by=list(euUK$pseudonym),FUN=sum)
summary(individual.counts)
euUK = euUK[which(euUK$pseudonym %in% individual.counts$Group.1[which(individual.counts$x>10)]),]
euUK = na.omit(euUK)

####################
### PCA on BERT encoding
### interpreting dimensions and "who reads what"
for.PCA = euUK[,which(str_detect(names(euUK),"^X"))]
res = PCA(for.PCA)
dim = as.data.frame(res$ind$coord)
dim$index = 1:nrow(dim)
dim$pseudonym = euUK$pseudonym

### interpreting PCA dimensions
source("./euro-tracking/src/01_prep/0_2_6_1_helper_fun_topic_extraction.R")

### dimension 1: football vs scary news (abuse & crime)
interpret(dim,1,euUK)
### dimension 2: TV series vs Brexit
interpret(dim,2,euUK)
### dimension 3: Europen Elections vs (sex) crime
interpret(dim,3,euUK)
### dimension 4: Local news vs celebrities
interpret(dim,4,euUK)
### dimension 5: sports (non football, mixed with some other international stuff) vs deals (money related, business)
interpret(dim,5,euUK)

load("./data/orig/survey_data.RData")
mix = merge(dim,survey_data,by="pseudonym",all.x=TRUE)
mix$v_31 = factor(mix$v_31)
levels(mix$v_31) = c(NA,NA,NA,"remain","leave")
names(mix)[which(names(mix)=="v_31")]="referendum_vote"

### variable "change" says whether they changed their mind in the last European elections (pre-election vs post-election survey)
### variable "referendum_vote" says what they said they voted (if they voted) to the 2016 EU membership referendum

### referendum x topics: leave people are more into scary news, more into brexit news, and more into sex crime as well
aggregate(mix[,paste0("Dim.",c(1:5))],by=list(mix$referendum_vote),FUN=mean)

summary(lm(data = mix, family = gaussian, as.numeric(referendum_vote) ~ Dim.1 + Dim.2 + Dim.3 + Dim.4 + Dim.5 ))
summary(lm(data = mix, family = gaussian, as.numeric(as.factor(change)) ~ Dim.1 + Dim.2 + Dim.3 + Dim.4 + Dim.5 ))

################# determination du nombre optimal de cluster
encodings = euUK[,which(str_detect(names(euUK),"^X"))]
wss <- 2:50
for (i in 2:50) wss[i-1] <- sum(kmeans(encodings,
                                       centers=i)$withinss)
plot(2:50, wss, type="b", xlab="Number of Clusters",
     ylab="Within groups sum of squares")
################# creation des clusters dim 1 et 2 aprÃ¨s normalisation

nbclust <- 50
kmeans.comment <- kmeans(encodings,nbclust)
cluster.kmeans <- as.factor(kmeans.comment$cluster)
table(cluster.kmeans)
centers = kmeans.comment$centers
nrow(centers)

ac = data.frame(clusters = cluster.kmeans,pseudonym = euUK$pseudonym)
levels(ac$clusters) =c("general news","sports","Brexit","TV series","money","celebs","crime","football","entertainment")
ac$count = 1
ac$used_at <- euUK$used_at
ac$url <- euUK$url

url_UK.small <- url_UK %>% 
  select(panelist_id, url, used_at, active_seconds) %>% 
  rename(pseudonym = panelist_id)

AC <- merge(ac, url_UK.small, by=c("pseudonym", "url", "used_at"), all.x = TRUE)

AC.2 <- AC %>% 
  group_by(pseudonym, clusters) %>% 
  summarise(
    n_per_clust = n(),
    d_per_clust = sum(active_seconds, na.rm = TRUE))

AC.n <- spread(AC.2, clusters, n_per_clust)
AC.d <- spread(AC.2, clusters, d_per_clust)

AC.n <- AC.n %>%
  group_by(pseudonym) %>% 
  summarise(
    clus1 = sum(`1`, na.rm = TRUE),
    clus2 = sum(`2`, na.rm = TRUE),
    clus3 = sum(`3`, na.rm = TRUE),
    clus4 = sum(`4`, na.rm = TRUE),
    clus5 = sum(`5`, na.rm = TRUE),
    clus6 = sum(`6`, na.rm = TRUE),
    clus7 = sum(`7`, na.rm = TRUE),
    clus8 = sum(`8`, na.rm = TRUE),
    clus9 = sum(`9`, na.rm = TRUE),
    clus10 = sum(`10`, na.rm = TRUE),
    clus11 = sum(`11`, na.rm = TRUE),
    clus12 = sum(`12`, na.rm = TRUE),
    clus13 = sum(`13`, na.rm = TRUE),
    clus14 = sum(`14`, na.rm = TRUE),
    clus15 = sum(`15`, na.rm = TRUE),
    clus16 = sum(`16`, na.rm = TRUE),
    clus17 = sum(`17`, na.rm = TRUE),
    clus18 = sum(`18`, na.rm = TRUE),
    clus19 = sum(`19`, na.rm = TRUE),
    clus20 = sum(`20`, na.rm = TRUE),
    clus21 = sum(`21`, na.rm = TRUE),
    clus22 = sum(`22`, na.rm = TRUE),
    clus23 = sum(`23`, na.rm = TRUE),
    clus24 = sum(`24`, na.rm = TRUE),
    clus25 = sum(`25`, na.rm = TRUE),
    clus26 = sum(`26`, na.rm = TRUE),
    clus27 = sum(`27`, na.rm = TRUE),
    clus28 = sum(`28`, na.rm = TRUE),
    clus29 = sum(`29`, na.rm = TRUE),
    clus30 = sum(`30`, na.rm = TRUE),
    clus31 = sum(`31`, na.rm = TRUE),
    clus32 = sum(`32`, na.rm = TRUE),
    clus33= sum(`33`, na.rm = TRUE),
    clus34= sum(`34`, na.rm = TRUE),
    clus35= sum(`35`, na.rm = TRUE),
    clus36= sum(`36`, na.rm = TRUE),
    clus37= sum(`37`, na.rm = TRUE),
    clus38= sum(`38`, na.rm = TRUE),
    clus39= sum(`39`, na.rm = TRUE),
    clus40= sum(`40`, na.rm = TRUE),
    clus41= sum(`41`, na.rm = TRUE),
    clus42= sum(`42`, na.rm = TRUE),
    clus43= sum(`43`, na.rm = TRUE),
    clus44= sum(`44`, na.rm = TRUE),
    clus45= sum(`45`, na.rm = TRUE),
    clus46= sum(`46`, na.rm = TRUE),
    clus47= sum(`47`, na.rm = TRUE),
    clus48= sum(`48`, na.rm = TRUE),
    clus49= sum(`49`, na.rm = TRUE),
    clus50= sum(`50`, na.rm = TRUE)
  )

AC.d <- AC.d %>%
  group_by(pseudonym) %>% 
  summarise(
    clus1 = sum(`1`, na.rm = TRUE),
    clus2 = sum(`2`, na.rm = TRUE),
    clus3 = sum(`3`, na.rm = TRUE),
    clus4 = sum(`4`, na.rm = TRUE),
    clus5 = sum(`5`, na.rm = TRUE),
    clus6 = sum(`6`, na.rm = TRUE),
    clus7 = sum(`7`, na.rm = TRUE),
    clus8 = sum(`8`, na.rm = TRUE),
    clus9 = sum(`9`, na.rm = TRUE),
    clus10 = sum(`10`, na.rm = TRUE),
    clus11 = sum(`11`, na.rm = TRUE),
    clus12 = sum(`12`, na.rm = TRUE),
    clus13 = sum(`13`, na.rm = TRUE),
    clus14 = sum(`14`, na.rm = TRUE),
    clus15 = sum(`15`, na.rm = TRUE),
    clus16 = sum(`16`, na.rm = TRUE),
    clus17 = sum(`17`, na.rm = TRUE),
    clus18 = sum(`18`, na.rm = TRUE),
    clus19 = sum(`19`, na.rm = TRUE),
    clus20 = sum(`20`, na.rm = TRUE),
    clus21 = sum(`21`, na.rm = TRUE),
    clus22 = sum(`22`, na.rm = TRUE),
    clus23 = sum(`23`, na.rm = TRUE),
    clus24 = sum(`24`, na.rm = TRUE),
    clus25 = sum(`25`, na.rm = TRUE),
    clus26 = sum(`26`, na.rm = TRUE),
    clus27 = sum(`27`, na.rm = TRUE),
    clus28 = sum(`28`, na.rm = TRUE),
    clus29 = sum(`29`, na.rm = TRUE),
    clus30 = sum(`30`, na.rm = TRUE),
    clus31 = sum(`31`, na.rm = TRUE),
    clus32 = sum(`32`, na.rm = TRUE),
    clus33= sum(`33`, na.rm = TRUE),
    clus34= sum(`34`, na.rm = TRUE),
    clus35= sum(`35`, na.rm = TRUE),
    clus36= sum(`36`, na.rm = TRUE),
    clus37= sum(`37`, na.rm = TRUE),
    clus38= sum(`38`, na.rm = TRUE),
    clus39= sum(`39`, na.rm = TRUE),
    clus40= sum(`40`, na.rm = TRUE),
    clus41= sum(`41`, na.rm = TRUE),
    clus42= sum(`42`, na.rm = TRUE),
    clus43= sum(`43`, na.rm = TRUE),
    clus44= sum(`44`, na.rm = TRUE),
    clus45= sum(`45`, na.rm = TRUE),
    clus46= sum(`46`, na.rm = TRUE),
    clus47= sum(`47`, na.rm = TRUE),
    clus48= sum(`48`, na.rm = TRUE),
    clus49= sum(`49`, na.rm = TRUE),
    clus50= sum(`50`, na.rm = TRUE)
  )

saveRDS(AC.d, "./data/work/dcluster_UK.RDS")
saveRDS(AC.n, "./data/work/ncluster_UK.RDS")


setwd("/home/r_uma_2019/respondi_eu/")

# Predictors: Domains

dur.dom.de <- readRDS("./data/work/dur_dom_de.RDS")
fre.dom.de <- readRDS("./data/work/fre_dom_de.RDS")
dur.dom.de[is.na(dur.dom.de)] <- 0
fre.dom.de[is.na(fre.dom.de)] <- 0


dur.dom.fr <- readRDS("./data/work/dur_dom_fr.RDS")
fre.dom.fr <- readRDS("./data/work/fre_dom_fr.RDS")
dur.dom.fr[is.na(dur.dom.fr)] <- 0
fre.dom.fr[is.na(fre.dom.fr)] <- 0


dur.dom.uk <- readRDS("./data/work/dur_dom_uk.RDS")
fre.dom.uk <- readRDS("./data/work/fre_dom_uk.RDS")
dur.dom.uk[is.na(dur.dom.uk)] <- 0
fre.dom.uk[is.na(fre.dom.uk)] <- 0

# Predictors: Apps

dur.app.de <- readRDS("./data/work/dur_app_de.RDS")
fre.app.de <- readRDS("./data/work/fre_app_de.RDS")
dur.app.de[is.na(dur.app.de)] <- 0
fre.app.de[is.na(fre.app.de)] <- 0

dur.app.fr <- readRDS("./data/work/dur_app_fr.RDS")
fre.app.fr <- readRDS("./data/work/fre_app_fr.RDS")
dur.app.fr[is.na(dur.app.fr)] <- 0
fre.app.fr[is.na(fre.app.fr)] <- 0

dur.app.uk <- readRDS("./data/work/dur_app_uk.RDS")
fre.app.uk <- readRDS("./data/work/fre_app_uk.RDS")
dur.app.uk[is.na(dur.app.uk)] <- 0
fre.app.uk[is.na(fre.app.uk)] <- 0

# Predictors: Categories

cat.app.de <- readRDS("./data/work/dur_per_cat_app_de.RDS")
cat.dom.de <- readRDS("./data/work/dur_per_cat_dom_de.RDS")

cat.app.de[is.na(cat.app.de)] <- 0
cat.dom.de[is.na(cat.dom.de)] <- 0


cat.app.fr <- readRDS("./data/work/dur_per_cat_app_fr.RDS")
cat.dom.fr <- readRDS("./data/work/dur_per_cat_dom_fr.RDS")

cat.app.fr[is.na(cat.app.fr)] <- 0
cat.dom.fr[is.na(cat.dom.fr)] <- 0


cat.app.uk <- readRDS("./data/work/dur_per_cat_app_uk.RDS")
cat.dom.uk <- readRDS("./data/work/dur_per_cat_dom_uk.RDS")

cat.app.uk[is.na(cat.app.uk)] <- 0
cat.dom.uk[is.na(cat.dom.uk)] <- 0

# Predictors: bert clusters
d.clust.de <- readRDS("./data/work/dcluster_DE.RDS")
d.clust.fr <- readRDS("./data/work/dcluster_FR.RDS")
d.clust.uk <- readRDS("./data/work/dcluster_UK.RDS")

n.clust.de <- readRDS("./data/work/ncluster_DE.RDS")
n.clust.fr <- readRDS("./data/work/ncluster_FR.RDS")
n.clust.uk <- readRDS("./data/work/ncluster_UK.RDS")

names(d.clust.de)[2:length(d.clust.de)] <- paste0("d.", names(d.clust.de[2:length(d.clust.de)]))
names(d.clust.fr)[2:length(d.clust.fr)] <- paste0("d.", names(d.clust.fr[2:length(d.clust.fr)]))
names(d.clust.uk)[2:length(d.clust.uk)] <- paste0("d.", names(d.clust.uk[2:length(d.clust.uk)]))

names(n.clust.de)[2:length(n.clust.de)] <- paste0("n.", names(n.clust.de[2:length(n.clust.de)]))
names(n.clust.fr)[2:length(n.clust.fr)] <- paste0("n.", names(n.clust.fr[2:length(n.clust.fr)]))
names(n.clust.uk)[2:length(n.clust.uk)] <- paste0("n.", names(n.clust.uk[2:length(n.clust.uk)]))

clust.de <- merge(d.clust.de,n.clust.de)
names(clust.de)[names(clust.de)=="pseudonym"] <- "panelist_id"
clust.fr <- merge(d.clust.fr,n.clust.fr)
names(clust.fr)[names(clust.fr)=="pseudonym"] <- "panelist_id"
clust.uk <- merge(d.clust.uk,n.clust.uk)
names(clust.uk)[names(clust.uk)=="pseudonym"] <- "panelist_id"

### DE
# dom predictors, dom users only
predictors.dom.de <- merge(dur.dom.de, fre.dom.de, by="panelist_id", all.x=TRUE)
# categories predictors, dom users only
predictors.cat.dom.de <- merge(cat.dom.de, predictors.dom.de[which(names(predictors.dom.de)=="panelist_id")], by="panelist_id", all.y=TRUE)

# app predictors, app users only
predictors.app.de <- merge(dur.app.de, fre.app.de, by="panelist_id", all.y=TRUE)
# categories predictors, app users only
predictors.cat.app.de <- merge(cat.app.de, predictors.app.de[which(names(predictors.app.de)=="panelist_id")], by="panelist_id", all.y=TRUE)

# bert predictors, bert users only
predictors.bert.de <- clust.de

### FR
# dom predictors, dom users only
predictors.dom.fr <- merge(dur.dom.fr, fre.dom.fr, by="panelist_id", all.x=TRUE)
# categories predictors, dom users only
predictors.cat.dom.fr <- merge(cat.dom.fr, predictors.dom.fr[which(names(predictors.dom.fr)=="panelist_id")], by="panelist_id", all.y=TRUE)

# app predictors, app users only
predictors.app.fr <- merge(dur.app.fr, fre.app.fr, by="panelist_id", all.y=TRUE)
# categories predictors, app users only
predictors.cat.app.fr <- merge(cat.app.fr, predictors.app.fr[which(names(predictors.app.fr)=="panelist_id")], by="panelist_id", all.y=TRUE)

# bert predictors, bert users only
predictors.bert.fr <- clust.fr

### UK
# dom predictors, dom users only
predictors.dom.uk <- merge(dur.dom.uk, fre.dom.uk, by="panelist_id", all.x=TRUE)
# categories predictors, dom users only
predictors.cat.dom.uk <- merge(cat.dom.uk, predictors.dom.uk[which(names(predictors.dom.uk)=="panelist_id")], by="panelist_id", all.y=TRUE)

# app predictors, app users only
predictors.app.uk <- merge(dur.app.uk, fre.app.uk, by="panelist_id", all.y=TRUE)
# categories predictors, app users only
predictors.cat.app.uk <- merge(cat.app.uk, predictors.app.uk[which(names(predictors.app.uk)=="panelist_id")], by="panelist_id", all.y=TRUE)

# bert predictors, bert users only
predictors.bert.uk <- clust.uk

# convert to numeric in case there are factors
# de
indx <- sapply(predictors.app.de, is.factor)
predictors.app.de[indx] <- lapply(predictors.app.de[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.dom.de, is.factor)
predictors.dom.de[indx] <- lapply(predictors.dom.de[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.bert.de, is.factor)
predictors.bert.de[indx] <- lapply(predictors.bert.de[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.cat.app.de, is.factor)
predictors.cat.app.de[indx] <- lapply(predictors.cat.app.de[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.cat.dom.de, is.factor)
predictors.cat.dom.de[indx] <- lapply(predictors.cat.dom.de[indx], function(x) as.numeric(as.character(x)))
rm(indx)

# fr
indx <- sapply(predictors.app.fr, is.factor)
predictors.app.fr[indx] <- lapply(predictors.app.fr[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.dom.fr, is.factor)
predictors.dom.fr[indx] <- lapply(predictors.dom.fr[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.bert.fr, is.factor)
predictors.bert.fr[indx] <- lapply(predictors.bert.fr[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.cat.app.fr, is.factor)
predictors.cat.app.fr[indx] <- lapply(predictors.cat.app.fr[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.cat.dom.fr, is.factor)
predictors.cat.dom.fr[indx] <- lapply(predictors.cat.dom.fr[indx], function(x) as.numeric(as.character(x)))
rm(indx)

# uk
indx <- sapply(predictors.app.uk, is.factor)
predictors.app.uk[indx] <- lapply(predictors.app.uk[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.dom.uk, is.factor)
predictors.dom.uk[indx] <- lapply(predictors.dom.uk[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.bert.uk, is.factor)
predictors.bert.uk[indx] <- lapply(predictors.bert.uk[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.cat.app.uk, is.factor)
predictors.cat.app.uk[indx] <- lapply(predictors.cat.app.uk[indx], function(x) as.numeric(as.character(x)))
rm(indx)
indx <- sapply(predictors.cat.dom.uk, is.factor)
predictors.cat.dom.uk[indx] <- lapply(predictors.cat.dom.uk[indx], function(x) as.numeric(as.character(x)))
rm(indx)


# change any NAs to 0 ... NAs result from merge of much smaller app dataset with larger domains dataset
predictors.app.uk[is.na(predictors.app.uk)] <- 0
predictors.app.fr[is.na(predictors.app.fr)] <- 0
predictors.app.de[is.na(predictors.app.de)] <- 0

predictors.cat.app.uk[is.na(predictors.cat.app.uk)] <- 0
predictors.cat.app.fr[is.na(predictors.cat.app.fr)] <- 0
predictors.cat.app.de[is.na(predictors.cat.app.de)] <- 0

predictors.cat.dom.uk[is.na(predictors.cat.dom.uk)] <- 0
predictors.cat.dom.fr[is.na(predictors.cat.dom.fr)] <- 0
predictors.cat.dom.de[is.na(predictors.cat.dom.de)] <- 0

predictors.dom.uk[is.na(predictors.dom.uk)] <- 0
predictors.dom.fr[is.na(predictors.dom.fr)] <- 0
predictors.dom.de[is.na(predictors.dom.de)] <- 0

predictors.bert.uk[is.na(predictors.bert.uk)] <- 0
predictors.bert.fr[is.na(predictors.bert.fr)] <- 0
predictors.bert.de[is.na(predictors.bert.de)] <- 0


saveRDS(predictors.bert.uk, file="./data/work/pred_track_bert_uk.RDS")
saveRDS(predictors.bert.fr, file="./data/work/pred_track_bert_fr.RDS")
saveRDS(predictors.bert.de, file="./data/work/pred_track_bert_de.RDS")

saveRDS(predictors.dom.uk, file="./data/work/pred_track_dom_uk.RDS")
saveRDS(predictors.dom.fr, file="./data/work/pred_track_dom_fr.RDS")
saveRDS(predictors.dom.de, file="./data/work/pred_track_dom_de.RDS")

saveRDS(predictors.cat.dom.uk, file="./data/work/pred_track_cat_dom_uk.RDS")
saveRDS(predictors.cat.dom.fr, file="./data/work/pred_track_cat_dom_fr.RDS")
saveRDS(predictors.cat.dom.de, file="./data/work/pred_track_cat_dom_de.RDS")

saveRDS(predictors.app.uk, file="./data/work/pred_track_app_uk.RDS")
saveRDS(predictors.app.fr, file="./data/work/pred_track_app_fr.RDS")
saveRDS(predictors.app.de, file="./data/work/pred_track_app_de.RDS")

saveRDS(predictors.cat.app.uk, file="./data/work/pred_track_cat_app_uk.RDS")
saveRDS(predictors.cat.app.fr, file="./data/work/pred_track_cat_app_fr.RDS")
saveRDS(predictors.cat.app.de, file="./data/work/pred_track_cat_app_de.RDS")

```





```{python LDA}
# imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
# load UK data
uk_df = pd.read_csv("scrapUK_all.csv", sep=';')
uk_df = uk_df.dropna()
###### LDA using filtered title ######
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')
document_term_mat_filter = vectorizer.fit_transform(uk_df['title_filtered'].values.astype('U'))
######## vocabulary size - 9287 elements (number of cols) ########
# apply LDA
from sklearn.decomposition import LatentDirichletAllocation
# try 5 components
LDA = LatentDirichletAllocation(n_components=5, random_state=15)
LDA.fit(document_term_mat_filter)
# add topics to document term matrix
topic_values = LDA.transform(document_term_mat_filter)
uk_df['topic_filter'] = topic_values.argmax(axis=1)

print('Using LDA with count vectorizer and the filtered titles,')
for i,topic in enumerate(LDA.components_):
    print(f'10 most likely words for topic {i}:')
    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])
    print('\n')

######## LDA using regular title ########
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')
document_term_mat_reg = vectorizer.fit_transform(uk_df['title'].values.astype('U'))
############ vocabulary size - 9328 elements (number of cols) ##########
# apply LDA
from sklearn.decomposition import LatentDirichletAllocation
# try 5 components
LDA = LatentDirichletAllocation(n_components=5, random_state=15)
LDA.fit(document_term_mat_reg)
# add topics to document term matrix
topic_values = LDA.transform(document_term_mat_reg)
uk_df['topic_reg'] = topic_values.argmax(axis=1)

print('Using LDA with count vectorizer and the regular titles,')
for i,topic in enumerate(LDA.components_):
    print(f'10 most likely words for topic {i}:')
    print([vectorizer.get_feature_names()[i] for i in topic.argsort()[-10:]])
    print('\n')
```

```{python NMF}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# NMF with filtered titles (combined with Tfidf)
# load data
uk_df2 = pd.read_csv("scrapUK_all.csv", sep=';')
uk_df2 = uk_df2.dropna()

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')
document_term_mat_filter = tfidf_vect.fit_transform(uk_df2['title_filtered'].values.astype('U'))

from sklearn.decomposition import NMF
nmf = NMF(n_components=5, random_state=15)
nmf.fit(document_term_mat_filter)

# concat topics
topic_nmf_filter = nmf.transform(document_term_mat_filter)
uk_df2['topic_filter'] = topic_nmf_filter.argmax(axis=1)

print('Using NMF with TFIDF vectorizer and the filtered titles,')
for i,topic in enumerate(nmf.components_):
    print(f'10 most likely words for topic {i}:')
    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])
    print('\n')

# NMF with regular titles (combined with Tfidf)
uk_df2 = pd.read_csv("scrapUK_all.csv", sep=';')
uk_df2 = uk_df2.dropna()

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_vect = TfidfVectorizer(max_df=0.8, min_df=2, stop_words='english')
document_term_mat_reg = tfidf_vect.fit_transform(uk_df2['title'].values.astype('U'))

from sklearn.decomposition import NMF
nmf = NMF(n_components=5, random_state=15)
nmf.fit(document_term_mat_reg)

# concat topics
topic_nmf_reg = nmf.transform(document_term_mat_reg)
uk_df2['topic_reg'] = topic_nmf_reg.argmax(axis=1)

print('Using NMF with TFIDF vectorizer and the regular titles,')
for i,topic in enumerate(nmf.components_):
    print(f'10 most likely words for topic {i}:')
    print([tfidf_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])
    print('\n')
```



